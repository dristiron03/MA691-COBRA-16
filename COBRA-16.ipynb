{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble._forest import BaseForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.tree import BaseDecisionTree\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_X_y\n",
    "#from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn import neighbors, tree, svm\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import numbers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTE(object):\n",
    "    \"\"\"Implementation of Synthetic Minority Over-Sampling Technique (SMOTE).\n",
    "    SMOTE performs oversampling of the minority class by picking target \n",
    "    minority class samples and their nearest minority class neighbors and \n",
    "    generating new samples that linearly combine features of each target \n",
    "    sample with features of its selected minority class neighbors [1].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of nearest neighbors.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, K. W. Bowyer, L. O. Hall, and P. Kegelmeyer. \"SMOTE:\n",
    "           Synthetic Minority Over-Sampling Technique.\" Journal of Artificial\n",
    "           Intelligence Research (JAIR), 2002.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k_neighbors=5, random_state=None):\n",
    "        self.k = k_neighbors\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Train model based on input data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_minority_samples, n_features]\n",
    "            Holds the minority samples.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.n_minority_samples, self.n_features = self.X.shape\n",
    "\n",
    "        # Learn nearest neighbors.\n",
    "        self.neigh = NearestNeighbors(n_neighbors=self.k + 1)\n",
    "        self.neigh.fit(self.X)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Generate samples.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of new synthetic samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        S : array, shape = [n_samples, n_features]\n",
    "            Returns synthetic samples.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed=self.random_state)\n",
    "        \n",
    "        S = np.zeros(shape=(n_samples, self.n_features))\n",
    "        \n",
    "        #Calculate synthetic samples.\n",
    "        for i in range(n_samples):\n",
    "            j = np.random.randint(0, self.X.shape[0])\n",
    "            \n",
    "            # Find the k NN for sample j.\n",
    "            # Exclude the sample itself.\n",
    "            nn = self.neigh.kneighbors(self.X[j].reshape(1, -1),\n",
    "                                       return_distance=False)[:, 1:]\n",
    "            nn_index = np.random.choice(nn[0])\n",
    "\n",
    "            diff = self.X[nn_index] - self.X[j]\n",
    "            gap = np.random.random()\n",
    "\n",
    "            S[i, :] = self.X[j, :] + gap * diff[:]\n",
    "        \n",
    "        return S\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTEBoost(AdaBoostClassifier):\n",
    "    \"\"\"Implementation of SMOTEBoost.\n",
    "    SMOTEBoost introduces data sampling into the AdaBoost algorithm by\n",
    "    oversampling the minority class using SMOTE on each boosting iteration [1].\n",
    "    This implementation inherits methods from the scikit-learn \n",
    "    AdaBoostClassifier class, only modifying the `fit` method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, optional (default=100)\n",
    "        Number of new synthetic samples per boosting step.\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of nearest neighbors.\n",
    "    base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required, as well as proper `classes_`\n",
    "        and `n_classes_` attributes.\n",
    "    n_estimators : int, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each classifier by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "    algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "        ``base_estimator`` must support calculation of class probabilities.\n",
    "        If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "        The SAMME.R algorithm typically converges faster than SAMME,\n",
    "        achieving a lower test error with fewer boosting iterations.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, A. Lazarevic, L. O. Hall, and K. W. Bowyer.\n",
    "           \"SMOTEBoost: Improving Prediction of the Minority Class in\n",
    "           Boosting.\" European Conference on Principles of Data Mining and\n",
    "           Knowledge Discovery (PKDD), 2003.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_samples=100,\n",
    "                 k_neighbors=5,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R',\n",
    "                 random_state=None):\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.algorithm = algorithm\n",
    "        self.smote = SMOTE(k_neighbors=k_neighbors,\n",
    "                           random_state=random_state)\n",
    "\n",
    "        super(SMOTEBoost, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, minority_target=None):\n",
    "        \"\"\"Build a boosted classifier/regressor from the training set (X, y),\n",
    "        performing SMOTE during each boosting step.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n",
    "            forced to DTYPE from tree._tree if the base classifier of this\n",
    "            ensemble weighted boosting classifier is a tree or forest.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape = [n_samples], optional\n",
    "            Sample weights. If None, the sample weights are initialized to\n",
    "            1 / n_samples.\n",
    "        minority_target : int\n",
    "            Minority class label.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        Notes\n",
    "        -----\n",
    "        Based on the scikit-learn v0.18 AdaBoostClassifier and\n",
    "        BaseWeightBoosting `fit` methods.\n",
    "        \"\"\"\n",
    "        # Check that algorithm is supported.\n",
    "        if self.algorithm not in ('SAMME', 'SAMME.R'):\n",
    "            raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n",
    "\n",
    "        # Check parameters.\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            DTYPE = np.float64  # from fast_dict.pxd\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n",
    "                         y_numeric=is_regressor(self))\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples.\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            # Normalize existing weights.\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive.\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Attempting to fit with a non-positive \"\n",
    "                    \"weighted number of samples.\")\n",
    "\n",
    "        if minority_target is None:\n",
    "            # Determine the minority class label.\n",
    "            stats_c_ = Counter(y)\n",
    "            maj_c_ = max(stats_c_, key=stats_c_.get)\n",
    "            min_c_ = min(stats_c_, key=stats_c_.get)\n",
    "            self.minority_target = min_c_\n",
    "        else:\n",
    "            self.minority_target = minority_target\n",
    "\n",
    "        # Check parameters.\n",
    "        self._validate_estimator()\n",
    "\n",
    "        # Clear any previous fit results.\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            X_min = X[np.where(y == self.minority_target)]\n",
    "\n",
    "            # SMOTE step.\n",
    "            if len(X_min) >= self.smote.k:\n",
    "                self.smote.fit(X_min)\n",
    "                X_syn = self.smote.sample(self.n_samples)\n",
    "                y_syn = np.full(X_syn.shape[0], fill_value=self.minority_target,\n",
    "                                dtype=np.int64)\n",
    "\n",
    "                # Normalize synthetic sample weights based on current training set.\n",
    "                sample_weight_syn = np.empty(X_syn.shape[0], dtype=np.float64)\n",
    "                sample_weight_syn[:] = 1. / X.shape[0]\n",
    "\n",
    "                # Combine the original and synthetic samples.\n",
    "                X = np.vstack((X, X_syn))\n",
    "                y = np.append(y, y_syn)\n",
    "\n",
    "                # Combine the weights.\n",
    "                sample_weight = \\\n",
    "                    np.append(sample_weight, sample_weight_syn).reshape(-1, 1)\n",
    "                sample_weight = \\\n",
    "                    np.squeeze(normalize(sample_weight, axis=0, norm='l1'))\n",
    "\n",
    "                # X, y, sample_weight = shuffle(X, y, sample_weight,\n",
    "                #                              random_state=random_state)\n",
    "\n",
    "            # Boosting step.\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state)\n",
    "\n",
    "            # Early termination.\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero.\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive.\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize.\n",
    "                sample_weight /= sample_weight_sum\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('pycobra.classifiercobra')\n",
    "\n",
    "\n",
    "class ClassifierCobra(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Classification algorithm as introduced by\n",
    "    Mojirsheibani [1999] Combining Classifiers via Discretization,\n",
    "    Journal of the American Statistical Association.\n",
    "    Parameters\n",
    "    ----------\n",
    "    random_state: integer or a numpy.random.RandomState object.\n",
    "        Set the state of the random number generator to pass on to shuffle and loading machines, to ensure\n",
    "        reproducibility of your experiments, for example.\n",
    "    Attributes\n",
    "    ----------\n",
    "    machines: A dictionary which maps machine names to the machine objects.\n",
    "            The machine object must have a predict method for it to be used during aggregation.\n",
    "    machine_predictions: A dictionary which maps machine name to it's predictions over X_l\n",
    "            This value is used to determine which points from y_l are used to aggregate.\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=None, machine_list='basic'):\n",
    "        self.random_state = random_state\n",
    "        self.machine_list = machine_list\n",
    "\n",
    "    def fit(self, X, y, default=True, X_k=None, X_l=None, y_k=None, y_l=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, [n_samples, n_features]\n",
    "            Training data which will be used to create ClassifierCobra.\n",
    "        y: array-like [n_samples]\n",
    "            Training labels for classification.\n",
    "        default: bool, optional\n",
    "            If set as true then sets up COBRA with default machines and splitting.\n",
    "        X_k : shape = [n_samples, n_features]\n",
    "            Training data which is used to train the machines loaded into COBRA.\n",
    "        y_k : array-like, shape = [n_samples]\n",
    "            Target values used to train the machines loaded into COBRA.\n",
    "        X_l : shape = [n_samples, n_features]\n",
    "            Training data which is used during the aggregation of COBRA.\n",
    "        y_l : array-like, shape = [n_samples]\n",
    "            Target values which are actually used in the aggregation of COBRA.\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.X_k_ = X_k\n",
    "        self.X_l_ = X_l\n",
    "        self.y_k_ = y_k\n",
    "        self.y_l_ = y_l\n",
    "        self.estimators_ = {}\n",
    "\n",
    "        # try block to pass scikit-learn estimator check.\n",
    "        try:\n",
    "            # set-up COBRA with default machines\n",
    "            if default:\n",
    "                self.split_data()\n",
    "                self.load_default(machine_list=self.machine_list)\n",
    "                self.load_machine_predictions()\n",
    "        except ValueError:\n",
    "            return self\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def pred(self, X, M, info=False):\n",
    "        \"\"\"\n",
    "        Performs the CLassififerCobra aggregation scheme, used in predict method.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, [n_features]\n",
    "        M: int, optional\n",
    "            M refers to the number of machines the prediction must be close to to be considered during aggregation.\n",
    "        info: boolean, optional\n",
    "            If info is true the list of points selected in the aggregation is returned.\n",
    "        Returns\n",
    "        -------\n",
    "        result: prediction\n",
    "        \"\"\"\n",
    "\n",
    "        # dictionary mapping machine to points selected\n",
    "        select = {}\n",
    "        for machine in self.estimators_:\n",
    "            # machine prediction\n",
    "            label = self.estimators_[machine].predict(X)\n",
    "            select[machine] = set()\n",
    "            # iterating from l to n\n",
    "            # replace with numpy iteration\n",
    "            for count in range(0, len(self.X_l_)):\n",
    "                if self.machine_predictions_[machine][count] == label:\n",
    "                    select[machine].add(count)\n",
    "\n",
    "        points = []\n",
    "        # count is the indice number.\n",
    "        for count in range(0, len(self.X_l_)):\n",
    "            # row check is number of machines which picked up a particular point\n",
    "            row_check = 0\n",
    "            for machine in select:\n",
    "                if count in select[machine]:\n",
    "                    row_check += 1\n",
    "            if row_check == M:\n",
    "                points.append(count)\n",
    "\n",
    "        # if no points are selected, return 0\n",
    "        if len(points) == 0:\n",
    "            if info:\n",
    "                logger.info(\"No points were selected, prediction is 0\")\n",
    "                return (0, 0)\n",
    "            logger.info(\"No points were selected, prediction is 0\")\n",
    "            return 0\n",
    "\n",
    "        # aggregate\n",
    "        classes = {}\n",
    "        for label in np.unique(self.y_l_):\n",
    "            classes[label] = 0\n",
    "\n",
    "        for point in points:\n",
    "            classes[self.y_l_[point]] += 1\n",
    "\n",
    "        result = int(max(classes, key=classes.get))\n",
    "        if info:\n",
    "            return result, points\n",
    "        return result\n",
    "\n",
    "\n",
    "    def predict(self, X, M=None, info=False):\n",
    "        \"\"\"\n",
    "        Performs the ClassifierCobra aggregation scheme, calls pred.\n",
    "        ClassifierCobra performs a majority vote among all points which are retained by the COBRA procedure.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, [n_features]\n",
    "        M: int, optional\n",
    "            M refers to the number of machines the prediction must be close to to be considered during aggregation.\n",
    "        info: boolean, optional\n",
    "            If info is true the list of points selected in the aggregation is returned.\n",
    "        Returns\n",
    "        -------\n",
    "        result: prediction\n",
    "        \"\"\"\n",
    "        X = check_array(X)\n",
    "\n",
    "        if M is None:\n",
    "            M = len(self.estimators_)\n",
    "        if X.ndim == 1:\n",
    "            return self.pred(X.reshape(1, -1), M=M)\n",
    "\n",
    "        result = np.zeros(len(X))\n",
    "        avg_points = 0\n",
    "        index = 0\n",
    "        for vector in X:\n",
    "            if info:\n",
    "                result[index], points = self.pred(vector.reshape(1, -1), M=M, info=info)\n",
    "                avg_points += len(points)\n",
    "            else:\n",
    "                result[index] = self.pred(vector.reshape(1, -1), M=M)\n",
    "            index += 1\n",
    "        \n",
    "        if info:\n",
    "            avg_points = avg_points / len(X_array)\n",
    "            return result, avg_points\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def predict_proba(self, X, kernel=None, metric=None, bandwidth=1, **kwargs): \n",
    "        \"\"\"\n",
    "        Performs the ClassifierCobra aggregation scheme and calculates probability of a point being in a particular class.\n",
    "        ClassifierCobra performs a majority vote among all points which are retained by the COBRA procedure.\n",
    "        \n",
    "        NOTE: this method is to visualise boundaries.\n",
    "        The current method is just the mean of the consituent machines, as the concept of that kind of predicted probability\n",
    "        doesn't exist (yet) for classifier cobra.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, [n_features]\n",
    "        \"\"\"\n",
    "\n",
    "        probs = []\n",
    "        for machine in self.estimators_:\n",
    "            try:\n",
    "                probs.append(self.estimators_[machine].predict_proba(X))\n",
    "            except AttributeError:\n",
    "                continue\n",
    "        prob = np.mean(probs, axis=0)\n",
    "        return prob\n",
    "\n",
    "\n",
    "    def split_data(self, k=None, l=None, shuffle_data=True):\n",
    "        \"\"\"\n",
    "        Split the data into different parts for training machines and for aggregation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int, optional\n",
    "            k is the number of points used to train the machines.\n",
    "            Those are the first k points of the data provided.\n",
    "        l: int, optional\n",
    "            l is the number of points used to form the ClassifierCobra aggregate.\n",
    "        shuffle: bool, optional\n",
    "            Boolean value to decide to shuffle the data before splitting.\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        if shuffle_data:\n",
    "            self.X_, self.y_ = shuffle(self.X_, self.y_, random_state=self.random_state)\n",
    "\n",
    "        if k is None and l is None:\n",
    "            k = int(len(self.X_) / 2)\n",
    "            l = int(len(self.X_))\n",
    "\n",
    "        if k is not None and l is None:\n",
    "            l = len(self.X_) - k\n",
    "\n",
    "        if l is not None and k is None:\n",
    "            k = len(self.X_) - l\n",
    "\n",
    "        self.X_k_ = self.X_[:k]\n",
    "        self.X_l_ = self.X_[k:l]\n",
    "        self.y_k_ = self.y_[:k]\n",
    "        self.y_l_ = self.y_[k:l]\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def load_default(self, machine_list='basic'):\n",
    "        \"\"\"\n",
    "        Loads 4 different scikit-learn regressors by default. The advanced list adds more machines. \n",
    "        As of current release SGD algorithm is not included in the advanced list.\n",
    "        Parameters\n",
    "        ----------\n",
    "        machine_list: optional, list of strings\n",
    "            List of default machine names to be loaded.\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        if machine_list == 'basic':\n",
    "            machine_list = ['sgd', 'tree', 'knn', 'svm']\n",
    "        \n",
    "        if machine_list == 'advanced':\n",
    "            machine_list = ['tree', 'knn', 'svm', 'logreg', 'naive_bayes', 'lda', 'neural_network']\n",
    "        \n",
    "        if machine_list == 'cobra':\n",
    "            machine_list = ['cobra', 'cobra', 'cobra', 'cobra', 'cobra', 'cobra']\n",
    "\n",
    "        for machine in machine_list:\n",
    "            try:\n",
    "                if machine == 'svm':\n",
    "                    self.estimators_['svm'] = svm.SVC().fit(self.X_k_, self.y_k_)\n",
    "                if machine == 'knn':\n",
    "                    self.estimators_['knn'] = neighbors.KNeighborsClassifier().fit(self.X_k_, self.y_k_)\n",
    "                if machine == 'tree':\n",
    "                    self.estimators_['tree'] = tree.DecisionTreeClassifier().fit(self.X_k_, self.y_k_)\n",
    "                if machine == 'logreg':\n",
    "                    self.estimators_['logreg'] = LogisticRegression(random_state=self.random_state).fit(self.X_k_, self.y_k_)\n",
    "                if machine == 'naive_bayes':\n",
    "                    self.estimators_['naive_bayes'] = GaussianNB().fit(self.X_k_, self.y_k_)\n",
    "                if machine == 'lda':\n",
    "                    self.estimators_['lda'] = LinearDiscriminantAnalysis().fit(self.X_k_, self.y_k_)\n",
    "                if machine == 'neural_network':\n",
    "                    self.estimators_['neural_network'] = MLPClassifier(random_state=self.random_state).fit(self.X_k_, self.y_k_)\n",
    "                if machine == 'cobra':\n",
    "                    self.estimators_['cobra'] = SMOTEBoost().fit(self.X_k_, self.y_k_)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def load_machine(self, machine_name, machine):\n",
    "        \"\"\"\n",
    "        Adds a machine to be used during the aggregation strategy.\n",
    "        The machine object must have been trained using X_k and y_k, and must have a 'predict()' method.\n",
    "        After the machine is loaded, for it to be used during aggregation, load_machine_predictions must be run.\n",
    "        Parameters\n",
    "        ----------\n",
    "        machine_name : string\n",
    "            Name of the machine you are loading\n",
    "        machine: machine/regressor object\n",
    "            The regressor machine object which is mapped to the machine_name\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        self.estimators_[machine_name] = machine\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def load_machine_predictions(self, predictions=None):\n",
    "        \"\"\"\n",
    "        Stores the trained machines' predicitons on D_l in a dictionary, to be used for predictions.\n",
    "        Should be run after all the machines to be used for aggregation is loaded.\n",
    "        Parameters\n",
    "        ----------\n",
    "        predictions: dictionary, optional\n",
    "            A pre-existing machine:predictions dictionary can also be loaded.\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        self.machine_predictions_ = {}\n",
    "        if predictions is None:\n",
    "            for machine in self.estimators_:\n",
    "                self.machine_predictions_[machine] = self.estimators_[machine].predict(self.X_l_)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def load_machine_proba_predictions(self, predictions=None):\n",
    "        \"\"\"\n",
    "        Stores the trained machines' predicitons on D_l in a dictionary, to be used for predictions.\n",
    "        Should be run after all the machines to be used for aggregation is loaded.\n",
    "        Parameters\n",
    "        ----------\n",
    "        predictions: dictionary, optional\n",
    "            A pre-existing machine:predictions dictionary can also be loaded.\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        self.machine_proba_predictions_ = {}\n",
    "        if predictions is None:\n",
    "            for machine in self.estimators_:\n",
    "                try:\n",
    "                    self.machine_proba_predictions_[machine] = self.estimators_[machine].predict_proba(self.X_l_)\n",
    "                except AttributeError:\n",
    "                    self.machine_proba_predictions_[machine] = self.estimators_[machine].decision_function(self.X_l_)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/Users/kanchanbakare/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/Users/kanchanbakare/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: pycobra in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (0.2.5)\n",
      "Requirement already satisfied: numpy in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from pycobra) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from pycobra) (0.24.2)\n",
      "Requirement already satisfied: matplotlib in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from pycobra) (3.2.1)\n",
      "Requirement already satisfied: seaborn in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from pycobra) (0.10.0)\n",
      "Requirement already satisfied: scipy in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from pycobra) (1.4.1)\n",
      "Requirement already satisfied: pandas in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from pycobra) (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from matplotlib->pycobra) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from matplotlib->pycobra) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from matplotlib->pycobra) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from matplotlib->pycobra) (2.4.6)\n",
      "Requirement already satisfied: six in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->pycobra) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->pycobra) (57.4.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from pandas->pycobra) (2019.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from scikit-learn->pycobra) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/kanchanbakare/anaconda3/lib/python3.7/site-packages (from scikit-learn->pycobra) (0.14.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/Users/kanchanbakare/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/Users/kanchanbakare/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/Users/kanchanbakare/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/Users/kanchanbakare/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/kanchanbakare/anaconda3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# from pycobra.classifiercobra import ClassifierCobra\n",
    "%pip install pycobra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pycobra.classifiercobra import ClassifierCobra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.estimator_checks import check_estimator\n",
    "# check_estimator(ClassifierCobra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "bc = datasets.load_breast_cancer()\n",
    "X = bc.data[:-20]\n",
    "y = bc.target[:-20]\n",
    "X_test = bc.data[-20:]\n",
    "y_test = bc.target[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = ClassifierCobra(machine_list='basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509971509971511"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "cc.fit(X, y)\n",
    "f1_score(y_test, cc.predict(X_test), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cobra SMOTE Boost f1 score: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"split\")\n",
    "cc = ClassifierCobra(machine_list='cobra')\n",
    "\n",
    "cc.fit(X, y)\n",
    "print(\"Cobra SMOTE Boost f1 score: \", end=\"\")\n",
    "f1_score(y_test, cc.predict(X_test), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('Class', data=df)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(18,4))\n",
    "\n",
    "amount_val = df['Amount'].values\n",
    "time_val = df['Time'].values\n",
    "\n",
    "sns.distplot(amount_val, ax=ax[0], color='r')\n",
    "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "sns.distplot(time_val, ax=ax[1], color='b')\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# RobustScaler is less prone to outliers.\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       1.783274    -0.994983 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.269825    -0.994983  1.191857  0.266151  0.166480  0.448154   \n",
       "2       4.983721    -0.994972 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       1.418291    -0.994972 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4       0.670579    -0.994960 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...       V20       V21       V22  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "\n",
    "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "# Amount and Time are Scaled!\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,new_x,y_train,new_y = train_test_split(df.drop('Class', axis=1),\n",
    "                                                 df['Class'],\n",
    "                                                 test_size=0.05,\n",
    "                                                 stratify= df['Class'],\n",
    "                                                 random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(new_x,\n",
    "                                                 new_y,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = ClassifierCobra(machine_list='cobra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cobra SMOTE Boost f1 score: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.fit(x_train, y_train)\n",
    "print(\"Cobra SMOTE Boost f1 score: \", end=\"\")\n",
    "f1_score(y_test, cc.predict(x_test), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified\n"
     ]
    }
   ],
   "source": [
    "x_train,new_x,y_train,new_y = train_test_split(df.drop('Class', axis=1),\n",
    "                                                 df['Class'],\n",
    "                                                 test_size=0.05,\n",
    "                                                 stratify= df['Class'],\n",
    "                                                 random_state=1)\n",
    "\n",
    "print(\"stratified\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(new_x,\n",
    "                                                 new_y,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cobra SMOTE Boost f1 score: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"split\")\n",
    "cc = ClassifierCobra(machine_list='cobra')\n",
    "\n",
    "cc.fit(x_train, y_train)\n",
    "print(\"Cobra SMOTE Boost f1 score: \", end=\"\")\n",
    "f1_score(y_test, cc.predict(x_test), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified\n",
      "split\n",
      "Cobra SMOTE Boost f1 score: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9992978349912229"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,new_x,y_train,new_y = train_test_split(df.drop('Class', axis=1),\n",
    "                                                 df['Class'],\n",
    "                                                 test_size=0.15,\n",
    "                                                 stratify= df['Class'],\n",
    "                                                 random_state=1)\n",
    "\n",
    "print(\"stratified\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(new_x,\n",
    "                                                 new_y,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=1)\n",
    "print(\"split\")\n",
    "cc = ClassifierCobra(machine_list='cobra')\n",
    "\n",
    "cc.fit(x_train, y_train)\n",
    "print(\"Cobra SMOTE Boost f1 score: \", end=\"\")\n",
    "f1_score(y_test, cc.predict(x_test), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified\n",
      "split\n",
      "Cobra SMOTE Boost f1 score: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9990395686250492"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,new_x,y_train,new_y = train_test_split(df.drop('Class', axis=1),\n",
    "                                                 df['Class'],\n",
    "                                                 test_size=0.25,\n",
    "                                                 stratify= df['Class'],\n",
    "                                                 random_state=1)\n",
    "\n",
    "print(\"stratified\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(new_x,\n",
    "                                                 new_y,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=1)\n",
    "print(\"split\")\n",
    "cc = ClassifierCobra(machine_list='cobra')\n",
    "\n",
    "cc.fit(x_train, y_train)\n",
    "print(\"Cobra SMOTE Boost f1 score: \", end=\"\")\n",
    "f1_score(y_test, cc.predict(x_test), average='weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
