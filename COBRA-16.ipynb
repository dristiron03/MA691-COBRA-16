{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble.forest import BaseForest\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.tree.tree import BaseDecisionTree\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import check_X_y\n",
    "#from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTE(object):\n",
    "    \"\"\"Implementation of Synthetic Minority Over-Sampling Technique (SMOTE).\n",
    "    SMOTE performs oversampling of the minority class by picking target \n",
    "    minority class samples and their nearest minority class neighbors and \n",
    "    generating new samples that linearly combine features of each target \n",
    "    sample with features of its selected minority class neighbors [1].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k_neighbors : int, optional (default=5)\n",
    "        Number of nearest neighbors.\n",
    "    random_state : int or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator.\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by np.random.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, K. W. Bowyer, L. O. Hall, and P. Kegelmeyer. \"SMOTE:\n",
    "           Synthetic Minority Over-Sampling Technique.\" Journal of Artificial\n",
    "           Intelligence Research (JAIR), 2002.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k_neighbors=5, random_state=None):\n",
    "        self.k = k_neighbors\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Train model based on input data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_minority_samples, n_features]\n",
    "            Holds the minority samples.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.n_minority_samples, self.n_features = self.X.shape\n",
    "\n",
    "        # Learn nearest neighbors.\n",
    "        self.neigh = NearestNeighbors(n_neighbors=self.k + 1)\n",
    "        self.neigh.fit(self.X)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"Generate samples.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int\n",
    "            Number of new synthetic samples.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        S : array, shape = [n_samples, n_features]\n",
    "            Returns synthetic samples.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed=self.random_state)\n",
    "        \n",
    "        S = np.zeroes(shape=(n_samples, self.n_features))\n",
    "        \n",
    "        #Calculate synthetic samples.\n",
    "        for i in range(n_samples):\n",
    "            j = np.random.randint(0, self.X.shape[0])\n",
    "            \n",
    "            # Find the k NN for sample j.\n",
    "            # Exclude the sample itself.\n",
    "            nn = self.neigh.kneighbors(self.X[j].reshape(1, -1),\n",
    "                                       return_distance=False)[:, 1:]\n",
    "            nn_index = np.random.choice(nn[0])\n",
    "\n",
    "            diff = self.X[nn_index] - self.X[j]\n",
    "            gap = np.random.random()\n",
    "\n",
    "            S[i, :] = self.X[j, :] + gap * diff[:]\n",
    "        \n",
    "        return S\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTEBoost(AdaBoostClassifier):\n",
    "        \"\"\"Implementation of SMOTEBoost.\n",
    "        SMOTEBoost introduces data sampling into the AdaBoost algorithm by\n",
    "        oversampling the minority class using SMOTE on each boosting iteration [1].\n",
    "        This implementation inherits methods from the scikit-learn \n",
    "        AdaBoostClassifier class, only modifying the `fit` method.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples : int, optional (default=100)\n",
    "            Number of new synthetic samples per boosting step.\n",
    "        k_neighbors : int, optional (default=5)\n",
    "            Number of nearest neighbors.\n",
    "        base_estimator : object, optional (default=DecisionTreeClassifier)\n",
    "            The base estimator from which the boosted ensemble is built.\n",
    "            Support for sample weighting is required, as well as proper `classes_`\n",
    "            and `n_classes_` attributes.\n",
    "        n_estimators : int, optional (default=50)\n",
    "            The maximum number of estimators at which boosting is terminated.\n",
    "            In case of perfect fit, the learning procedure is stopped early.\n",
    "        learning_rate : float, optional (default=1.)\n",
    "            Learning rate shrinks the contribution of each classifier by\n",
    "            ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "            ``n_estimators``.\n",
    "        algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n",
    "            If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
    "            ``base_estimator`` must support calculation of class probabilities.\n",
    "            If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
    "            The SAMME.R algorithm typically converges faster than SAMME,\n",
    "            achieving a lower test error with fewer boosting iterations.\n",
    "        random_state : int or None, optional (default=None)\n",
    "            If int, random_state is the seed used by the random number generator.\n",
    "            If None, the random number generator is the RandomState instance used\n",
    "            by np.random.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] N. V. Chawla, A. Lazarevic, L. O. Hall, and K. W. Bowyer.\n",
    "               \"SMOTEBoost: Improving Prediction of the Minority Class in\n",
    "               Boosting.\" European Conference on Principles of Data Mining and\n",
    "               Knowledge Discovery (PKDD), 2003.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self,\n",
    "                    n_samples=100,\n",
    "                    k_neighbors=5,\n",
    "                    base_estimator=None,\n",
    "                    n_estimators=50,\n",
    "                    learning_rate=1.,\n",
    "                    algorithm='SAMME.R',\n",
    "                    random_state=None):\n",
    "            \n",
    "            self.n_samples = n_samples\n",
    "            self.algorithm = algorithm\n",
    "            self.smote = SMOTE(k_neighbors=k_neighbors,\n",
    "                              random_state=random_state)\n",
    "            \n",
    "            super().__init__(\n",
    "                base_estimator=base_estimator,\n",
    "                n_estimators=n_estimators,\n",
    "                learning_rate=learning_rate,\n",
    "                random_state=random_state)\n",
    "            \n",
    "        def fit(self, X, y, sample_weight=None, minority_target=None):\n",
    "            \"\"\"Build a boosted classifier/regressor from the training set (X, y),\n",
    "            performing SMOTE during each boosting step.\n",
    "            Parameters\n",
    "            ----------\n",
    "            X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "                The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "                DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n",
    "                forced to DTYPE from tree._tree if the base classifier of this\n",
    "                ensemble weighted boosting classifier is a tree or forest.\n",
    "            y : array-like of shape = [n_samples]\n",
    "                The target values (class labels in classification, real numbers in\n",
    "                regression).\n",
    "            sample_weight : array-like of shape = [n_samples], optional\n",
    "                Sample weights. If None, the sample weights are initialized to\n",
    "                1 / n_samples.\n",
    "            minority_target : int\n",
    "                Minority class label.\n",
    "            Returns\n",
    "            -------\n",
    "            self : object\n",
    "                Returns self.\n",
    "            Notes\n",
    "            -----\n",
    "            Based on the scikit-learn v0.18 AdaBoostClassifier and\n",
    "            BaseWeightBoosting `fit` methods.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Check that algorithm is supported.\n",
    "            if self.algorithm not in ('SAMME', 'SAMME.R'):\n",
    "                raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n",
    "                \n",
    "            # Check parameters\n",
    "            if self.learning_rate <= 0:\n",
    "                raise ValueError(\"learning rate must be greater than zero\")\n",
    "                \n",
    "            if(self.base_estimator is None or\n",
    "                    isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                    BaseForest))):\n",
    "                DTYPE = np.float64 # from fast_dict.pxd\n",
    "                dtype = DTYPE\n",
    "                accept_sparse = 'csc'\n",
    "            else:\n",
    "                dtype = None\n",
    "                accept_sparse = ['csr', 'csc']\n",
    "                \n",
    "            X,y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n",
    "                           y_numeric=is_regressor(self))\n",
    "            \n",
    "            if sample_weight is None:\n",
    "                # Initialize weights to 1/n_samples.\n",
    "                sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "                sample_weight[:] = 1./X.shape[0]\n",
    "            else:\n",
    "                sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "                # Normalize existing weights\n",
    "                sample_weight = sample_weight/sample_weight.sum(dtype=np.float64)\n",
    "                \n",
    "                # Check that the sample weights sum is positive.\n",
    "                if sample_weight.sum() <= 0:\n",
    "                    raise ValueError(\n",
    "                        \"Attempting to fit with a non-positive \"\n",
    "                        \"weighted number of samples.\")\n",
    "                    \n",
    "                if minority_target is None:\n",
    "                    # Determine the minority class label.\n",
    "                    stats_c_ = Counter(y)\n",
    "                    maj_c_ = max(stats_c_, key=stats_c_.get)\n",
    "                    min_c_ = min(stats_c_, key=stats_c_.get)\n",
    "                    self.minority_target = min_c_\n",
    "                else:\n",
    "                    self.minority_target = minority_target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
